{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
        {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tharindudr/CS3MIR/blob/main/CS3MIR_Lab_05.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CS3MIR Lab 5. Document Clustering**"
      ],
      "metadata": {
        "id": "Ip892wRmvfAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, we will learn how to cluster a set of documents using Python. My motivating example is to identify the latent structures within the synopses of the top 100 films of all time. We will be performing the following steps in order to achieve the clustering.\n",
        "*   tokenizing and stemming each synopsis\n",
        "*   transforming the corpus into vector space using tf-idf\n",
        "*   calculating cosine distance between each document as a measure of similarity\n",
        "*   clustering the documents using the k-means algorithm\n",
        "*   using multidimensional scaling to reduce dimensionality within the corpus\n",
        "*   plotting the clustering output using matplotlib\n"
      ],
      "metadata": {
        "id": "Olq3ZgzewSaM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FHAbSRVgVjmn",
        "outputId": "1fa61f27-1eb4-45aa-b282-f41394f271ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "from sklearn import feature_extraction\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import MDS\n",
        "import joblib\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "import requests\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the dataset"
      ],
      "metadata": {
        "id": "36uOnU6FwTMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/TharinduDR/CS3MIR/raw/main/data/Lab5.zip\n",
        "!unzip Lab5.zip -d docs"
      ],
      "metadata": {
        "id": "5Qe_YflzwV7t",
        "outputId": "828a693c-83c7-41fa-fff0-9e4f12af2855",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-22 20:02:13--  https://github.com/TharinduDR/CS3MIR/raw/main/data/Lab5.zip\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/TharinduDR/CS3MIR/main/data/Lab5.zip [following]\n",
            "--2023-02-22 20:02:13--  https://raw.githubusercontent.com/TharinduDR/CS3MIR/main/data/Lab5.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 565526 (552K) [application/zip]\n",
            "Saving to: ‘Lab5.zip.2’\n",
            "\n",
            "Lab5.zip.2          100%[===================>] 552.27K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-02-22 20:02:13 (12.2 MB/s) - ‘Lab5.zip.2’ saved [565526/565526]\n",
            "\n",
            "Archive:  Lab5.zip\n",
            "replace docs/Lab5/genres_list.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "  inflating: docs/Lab5/genres_list.txt  \n",
            "  inflating: docs/Lab5/link_list.txt  \n",
            "  inflating: docs/Lab5/link_list_imdb.txt  \n",
            "  inflating: docs/Lab5/link_list_wiki.txt  \n",
            " extracting: docs/Lab5/synopses_list.txt  \n",
            "  inflating: docs/Lab5/synopses_list.txt.txt  \n",
            "  inflating: docs/Lab5/synopses_list_imdb.txt  \n",
            "  inflating: docs/Lab5/synopses_list_wiki.txt  \n",
            "  inflating: docs/Lab5/title_list.txt  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have three primary lists:\n",
        "\n",
        "1. 'titles': the titles of the films in their rank order\n",
        "2. 'wiki synopses': the synopses of the films from wiki matched to the 'titles' order\n",
        "3. 'imdb synopses': the synopses of the films from imdb matched to the 'titles' order\n",
        "\n",
        "The reading from file code is pretty simple - similar to previous workshops:"
      ],
      "metadata": {
        "id": "COcaZR9Czj9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import three lists: titles, wikipedia synopses and imdb synopses\n",
        "#by reading the data from files\n",
        "#ensure that we are reading only the first 100 records.\n",
        "titles = open('docs/Lab5/title_list.txt').read().split('\\n')\n",
        "#ensures that only the first 100 are read in\n",
        "titles = titles[:100]\n",
        "\n",
        "synopses_wiki = open('docs/Lab5/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_wiki = synopses_wiki[:100]\n",
        "\n",
        "synopses_clean_wiki = []\n",
        "for text in synopses_wiki:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_wiki.append(text)\n",
        "\n",
        "synopses_wiki = synopses_clean_wiki\n",
        "\n",
        "synopses_imdb = open('docs/Lab5/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
        "synopses_imdb = synopses_imdb[:100]\n",
        "\n",
        "synopses_clean_imdb = []\n",
        "\n",
        "for text in synopses_imdb:\n",
        "    text = BeautifulSoup(text, 'html.parser').getText()\n",
        "    #strips html formatting and converts to unicode\n",
        "    synopses_clean_imdb.append(text)\n",
        "\n",
        "synopses_imdb = synopses_clean_imdb\n",
        "\n",
        "print(str(len(titles)) + ' titles')\n",
        "print(str(len(synopses_wiki)) + ' wiki synopses')\n",
        "print(str(len(synopses_imdb)) + ' imdb synopses')"
      ],
      "metadata": {
        "id": "R8MkKa1Azw9i",
        "outputId": "4b14d5be-c450-4008-b108-9f41ad586c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 titles\n",
            "100 wiki synopses\n",
            "100 imdb synopses\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After reading the data from files, we are cleaning the synopses (wiki and imdb both) using BeautifulSoup. BeautifulSoup is a Python library that is used for web scraping purposes to pull the data out of HTML and XML files. It creates a parse tree from page source code that can be used to extract data in a hierarchical and more readable manner.\n",
        "\n",
        "Complete the following task: extract the Aston University wiki page (https://en.wikipedia.org/wiki/Aston_University), get the page content using BeautifulSoup, and print the text. You can get the page using the following command:\n",
        "page = requests.get(YOURLINK). Store the page text in astonText variable.\n",
        "  "
      ],
      "metadata": {
        "id": "6GuoHcPf2vA5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Mfgkr2i4hAj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, Let's combine the film wiki and imdb synopses and generate the rank for each."
      ],
      "metadata": {
        "id": "4sMTnz1L6-wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "synopses = []\n",
        "\n",
        "for i in range(len(synopses_wiki)):\n",
        "    item = synopses_wiki[i] + synopses_imdb[i]\n",
        "    synopses.append(item)\n",
        "\n",
        "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
        "ranks = []\n",
        "for i in range(0,len(titles)):\n",
        "    ranks.append(i)"
      ],
      "metadata": {
        "id": "pKT98Zry81xu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need to remove the stopwords and stemmer from synopses, therefore, we will be using the last workshop code."
      ],
      "metadata": {
        "id": "iOXt6HKn85pf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load nltk's English stopwords as variable called 'stopwords'\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def tokenize_and_stem(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
        "    return stems\n",
        "\n",
        "def tokenize_only(text):\n",
        "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
        "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
        "    filtered_tokens = []\n",
        "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
        "    for token in tokens:\n",
        "        if re.search('[a-zA-Z]', token):\n",
        "            filtered_tokens.append(token)\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "XBoqwIqM9Kdx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's call the function to process the synopses and store the proccessed (filtered) synopses in data frame."
      ],
      "metadata": {
        "id": "J16mlxku-NIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "totalvocab_stemmed = []\n",
        "totalvocab_tokenized = []\n",
        "for i in synopses:\n",
        "  allwords_stemmed = tokenize_and_stem(i)\n",
        "  totalvocab_stemmed.extend(allwords_stemmed)\n",
        "    \n",
        "  allwords_tokenized = tokenize_only(i)\n",
        "  totalvocab_tokenized.extend(allwords_tokenized)\n",
        "\n",
        "vocab_frame = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)"
      ],
      "metadata": {
        "id": "dVhHDRII-FvU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete the following task: remove the stopwords and stemmers from the astonText (This variable contains the Aston wiki page text).  "
      ],
      "metadata": {
        "id": "uysWHwo79n1N"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9VKZbYS2_VnP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tf-idf and document similarity** \n",
        "We will be using frequency-inverse document frequency (tf-idf) vectorizer parameters and then convert the synopses list into a tf-idf matrix. Please refer to Lab 02 if you are unsimilar with Tf-idf."
      ],
      "metadata": {
        "id": "BV41kTvd_nbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n",
        "                                 min_df=0.2, stop_words='english',\n",
        "                                 use_idf=True, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
        "\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(synopses)\n",
        "\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "terms = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "dist = 1 - cosine_similarity(tfidf_matrix)"
      ],
      "metadata": {
        "id": "oTrucuQ7AfYM",
        "outputId": "c20f205d-b13d-493d-974f-e1438fa4bafb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'forti', 'henc', 'hereaft', 'herebi', 'howev', 'hundr', 'inde', 'mani', 'meanwhil', 'moreov', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'togeth', 'twelv', 'twenti', 'veri', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 563)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering: Using the tf-idf matrix, we can run a slew of clustering algorithms to better understand the hidden structure within the synopses. We will be using k-means algorithm with cluster size 5. Each observation is assigned to a cluster based on the cluster sum of squares. Next, the mean of the clustered observations is calculated and used as the new cluster centroid. Then, observations are reassigned to clusters and centroids recalculated in an iterative process until the algorithm reaches convergence."
      ],
      "metadata": {
        "id": "NxmDLRzoAsHf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def cluster(num_clusters):\n",
        "    km = KMeans(n_clusters=num_clusters)\n",
        "\n",
        "    km.fit(tfidf_matrix)\n",
        "\n",
        "    clusters = km.labels_.tolist()\n",
        "\n",
        "    joblib.dump(km,'doc_cluster.pkl')\n",
        "    km = joblib.load('doc_cluster.pkl')\n",
        "    clusters = km.labels_.tolist()\n",
        "\n",
        "    films = { 'title': titles, 'rank': ranks, 'synopsis': synopses, 'cluster': clusters }\n",
        "\n",
        "    frame = pd.DataFrame(films, index = [clusters] , columns = ['rank', 'title', 'cluster'])\n",
        "    return frame, km\n",
        "\n",
        "num_clusters = 5\n",
        "frame, km =  cluster(num_clusters)"
      ],
      "metadata": {
        "id": "szau--O5BV-I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's display the number of films per cluster."
      ],
      "metadata": {
        "id": "LBf_8mWPB02Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frame['cluster'].value_counts()"
      ],
      "metadata": {
        "id": "4P_2pHoFB1b4",
        "outputId": "31051f7b-de37-4b80-de7d-5369c6ddd8f6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    42\n",
              "3    18\n",
              "4    17\n",
              "0    12\n",
              "1    11\n",
              "Name: cluster, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's groupby cluster for aggregation purposes and display the average rank (1 to 100) per cluster."
      ],
      "metadata": {
        "id": "lIshCNGSCEyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = frame['rank'].groupby(frame['cluster'])\n",
        "grouped.mean()"
      ],
      "metadata": {
        "id": "tgdqCQyvCFJ6",
        "outputId": "6d054bf4-cca9-4cfd-b4ac-94576f9afdcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "cluster\n",
              "0    47.583333\n",
              "1    48.000000\n",
              "2    55.095238\n",
              "3    44.333333\n",
              "4    43.470588\n",
              "Name: rank, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's identify the top n (here we are using 6) words per cluster that are nearest to the cluster centroid and display them. These words gives a good sense of the main topic of the cluster."
      ],
      "metadata": {
        "id": "hL--AP_5Cq_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "print(\"Top terms per cluster:\")\n",
        "print()\n",
        "order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(num_clusters):\n",
        "    print(\"Cluster %d words:\" % i, end='')\n",
        "    for ind in order_centroids[i, :6]:\n",
        "        print(' %s' % vocab_frame.loc[terms[ind].split(' ')].values.tolist()[0][0].encode('utf-8', 'ignore'), end=',')\n",
        "    print()\n",
        "    print()\n",
        "    print(\"Cluster %d titles:\" % i, end='')\n",
        "    for title in frame.loc[i]['title'].values.tolist():\n",
        "        print(' %s,' % title, end='')\n",
        "    print()\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "X2hU5uy86_aj",
        "outputId": "d39fad3a-4aab-4860-9bfb-8b6b887320dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top terms per cluster:\n",
            "\n",
            "Cluster 0 words: b'george', b'brothers', b'family', b'father', b'york', b'new',\n",
            "\n",
            "Cluster 0 titles: The Godfather, Raging Bull, The Godfather: Part II, It's a Wonderful Life, The Philadelphia Story, An American in Paris, The King's Speech, A Place in the Sun, Rain Man, Annie Hall, Tootsie, Yankee Doodle Dandy,\n",
            "\n",
            "Cluster 1 words: b'soldiers', b'killed', b'army', b'battles', b'men', b'orders',\n",
            "\n",
            "Cluster 1 titles: Forrest Gump, Gladiator, From Here to Eternity, Saving Private Ryan, Patton, Braveheart, Butch Cassidy and the Sundance Kid, Platoon, The Deer Hunter, All Quiet on the Western Front, Shane,\n",
            "\n",
            "Cluster 2 words: b'car', b'police', b'father', b'apartments', b'friends', b'love',\n",
            "\n",
            "Cluster 2 titles: The Shawshank Redemption, One Flew Over the Cuckoo's Nest, Citizen Kane, Psycho, Sunset Blvd., Vertigo, On the Waterfront, West Side Story, The Silence of the Lambs, Chinatown, Singin' in the Rain, Some Like It Hot, 12 Angry Men, Amadeus, Rocky, A Streetcar Named Desire, To Kill a Mockingbird, My Fair Lady, The Good, the Bad and the Ugly, The Apartment, Goodfellas, The Exorcist, The French Connection, It Happened One Night, Midnight Cowboy, Good Will Hunting, Terms of Endearment, Fargo, The Green Mile, Nashville, The Graduate, American Graffiti, Pulp Fiction, The Maltese Falcon, A Clockwork Orange, Taxi Driver, Wuthering Heights, Double Indemnity, Rebel Without a Cause, Rear Window, The Third Man, North by Northwest,\n",
            "\n",
            "Cluster 3 words: b'ship', b'captain', b'water', b'home', b'command', b'group',\n",
            "\n",
            "Cluster 3 titles: The Wizard of Oz, Titanic, The Sound of Music, Star Wars, E.T. the Extra-Terrestrial, 2001: A Space Odyssey, Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb, Apocalypse Now, The Lord of the Rings: The Return of the King, Raiders of the Lost Ark, Ben-Hur, Jaws, City Lights, Mr. Smith Goes to Washington, Close Encounters of the Third Kind, The African Queen, Stagecoach, Mutiny on the Bounty,\n",
            "\n",
            "Cluster 4 words: b'war', b'camps', b'family', b'army', b'killed', b'town',\n",
            "\n",
            "Cluster 4 titles: Schindler's List, Casablanca, Gone with the Wind, Lawrence of Arabia, The Bridge on the River Kwai, Gandhi, Unforgiven, The Best Years of Our Lives, Doctor Zhivago, The Treasure of the Sierra Madre, High Noon, Dances with Wolves, The Pianist, Out of Africa, Giant, The Grapes of Wrath, Network,\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Do the Kmeans clustering for four clusters and print the top terms per cluster"
      ],
      "metadata": {
        "id": "rQhBFX4irbCO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z20rS2P4DUPx"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}
